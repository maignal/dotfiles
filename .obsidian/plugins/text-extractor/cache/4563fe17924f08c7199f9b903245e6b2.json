{"path":"The Essential/Summaries/TheoryOfComputation_JoachimFavre.pdf","text":"Theory of computation Prof. Mika Göös — EPFL Notes by Joachim Favre Computer science bachelor — Semester 4 Spring 2023 I made this document for my own use, but I thought that typed notes might be of interest to others. So, I shared it (with you, if you are reading this!); since it did not cost me anything. I just ask you to keep in mind that there are mistakes, it is impossible not to make any. If you find some, please feel free to share them with me (grammatical and vocabulary errors are of course also welcome). You can contact me at the following e-mail address: joachim.favre@epfl.ch If you did not get this document through my GitHub repository, then you may be interested by the fact that I have one on which I put my typed notes. Here is the link (go take a look in the “Releases” section to find the compiled documents): https://github.com/JoachimFavre/EPFLNotesIN Please note that the content does not belong to me. I have made some structural changes, reworded some parts, and added some personal notes; but the wording and explanations come mainly from the Professor, and from the book on which they based their course. I think it is worth mentioning that in order to get these notes typed up, I took my notes in LATEXduring the course, and then made some corrections. I do not think typing handwritten notes is doable in terms of the amount of work. To take notes in LATEX, I took my inspiration from the following link, written by Gilles Castel. If you want more details, feel free to contact me at my e-mail address, mentioned hereinabove. https://castel.dev/post/lecture-notes-1/ I would also like to specify that the words “trivial” and “simple” do not have, in this course, the definition you find in a dictionary. We are at EPFL, nothing we do is trivial. Something trivial is something that a random person in the street would be able to do. In our context, understand these words more as “simpler than the rest”. Also, it is okay if you take a while to understand something that is said to be trivial (especially as I love using this word everywhere hihi). Since you are reading this, I will give you a little advice. Sleep is a much more powerful tool than you may imagine, so never neglect a good night of sleep in favour of studying (especially the night before the exam). I will also take the liberty of paraphrasing my high school philosophy teacher, Ms. Marques, I hope you will have fun during your exams! Version 2023–06–13 To Gilles Castel, whose work has inspired me this note taking method. Rest in peace, nobody deserves to go so young. Contents 1 Summary by lecture 11 2 Introduction 13 3 Finite automatons 15 3.1 Deterministic finite automaton . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.2 Non-deterministic finite automaton . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.3 Non-regular languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4 Turing machines 27 4.1 Formal definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.2 Decidable and undecidable languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.3 Reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 5 Complexity classes 37 5.1 P complexity class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.2 NP complexity class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5.3 Polynomial-time reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.4 Cook-Levin theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 7 Theory of computation CONTENTS 8 List of lectures Lecture 1 : Deterministic finite automatons — Monday 20 th February 2023 . . . . . . . . . 13 Lecture 2 : Killing cats — Monday 27th February 2023 . . . . . . . . . . . . . . . . . . . . . . . 18 Lecture 3 : ”Pigeon collisions” — Monday 6th March 2023 . . . . . . . . . . . . . . . . . . . . 24 Lecture 4 : Turing , — Monday 13 th March 2023 . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Lecture 5 : Halting problem — Monday 20 th March 2023 . . . . . . . . . . . . . . . . . . . . . 30 Lecture 6 : Reductions — Monday 27th March 2023 . . . . . . . . . . . . . . . . . . . . . . . . 33 Lecture 7 : P and NP — Monday 17th April 2023 . . . . . . . . . . . . . . . . . . . . . . . . . . 37 Lecture 8 : Turning satisfying assignments to graphs — Monday 24th April 2023 . . . . . . 40 Lecture 9 : I hope the rest of the course will not be like this — Monday 1 st May 2023 . . 44 Lecture 10 : More fun proofs — Monday 8 th May 2023 . . . . . . . . . . . . . . . . . . . . . . 48 Lecture 11 : I thought this would never happen — Monday 15th May 2023 . . . . . . . . . 51 9 Theory of computation LIST OF LECTURES 10 Chapter 1 Summary by lecture Lecture 1 : Deterministic finite automatons — Monday 20 th February 2023 p. 13 • Definition of deterministic finite automatons, and many examples. • Explanation of how to prove that a given language is the one accepted by the finite automaton, and some examples. Lecture 2 : Killing cats — Monday 27th February 2023 p. 18 • Definition of regular languages. • Proof that the complement of a regular language, and the union and intersection of any two regular languages, are also regular, and explanation of the DFA constructions. • Definition of NFAs. • Proof that any NFA is equivalent to a DFA, and explanation of the construction. • Proof that the concatenation of any two regular languages is also regular, and explanation of its NFA construction. Lecture 3 : ”Pigeon collisions” — Monday 6th March 2023 p. 24 • Explanation and proof of the pumping lemma. • Examples of application of the pumping lemma to show that some languages are not regular. Lecture 4 : Turing , — Monday 13 th March 2023 p. 27 • Definition of Turing machines and their configurations. • Definition of Turing-recognisable and Turing-decidable languages. Lecture 5 : Halting problem — Monday 20 th March 2023 p. 30 • Proof of the existence of undecidable languages. • Proof that HALT and AT M are undecidable. • Proof that HALT is unrecognisable. 11 Theory of computation CHAPTER 1. SUMMARY BY LECTURE Lecture 6 : Reductions — Monday 27th March 2023 p. 33 • Definition of reductions. • Proof of theorems allowing to show that some language is recognisable or decidable, if it reduces to some recognisable or decidable language. Lecture 7 : P and NP — Monday 17th April 2023 p. 37 • Definition of big-O and small-o. • Definition of TIME complexity classes. • Definition of polynomial time verifiable language. • Definition of P and NP complexity classes. Lecture 8 : Turning satisfying assignments to graphs — Monday 24th April 2023 p. 40 • Proof that SAT, GI and INDSET are in NP. • Explanation of why NP is called nondeterministic P. • Definition of polynomial-time reductions, and proof of some of their properties. • Definition of NP-hard and NP-complete problems. • Proof that INDSET is NP-complete. Lecture 9 : I hope the rest of the course will not be like this — Monday 1 st May 2023 p. 44 • Definition of kSat, and proof that it is NP-complete for k ≥ 3. • Definition of CLIQUE, and proof that it is NP-complete. • Definition of VERTEX COVER, and proof that it is NP-complete. • Definition of SET COVER, and proof that it is NP-complete. Lecture 10 : More fun proofs — Monday 8 th May 2023 p. 48 • Definition of PERFECT-3-MATCHING. • Proof that PERFECT-3-MATCHING is NP-complete. • Definition of SUBSET-SUM. • Proof that SUBSET-SUM is NP-complete. Lecture 11 : I thought this would never happen — Monday 15th May 2023 p. 51 • Formal definition of circuits. • Proof that any Turing Machine can be converted to a circuit efficiently. • Proof of Cook-Levin theorem. 12 Monday 20th February 2023 — Lecture 1 : Deterministic finite automatons Chapter 2 Introduction History In the 1930s, people were asking themselves what can be mathematically proved (meaning, whether is maths limited), and what can be computed. Gödel answered the first one, and Turing the second one. There are many people who work on theoretical computer science. Alan Turing is seen as the father of computer science: he defined mathematically what is an algorithm thanks to the Turing’s machine, and he immediately proved limitations of it. Jack Edmonds introduced the class P, algorithms that can be solved efficiently (meaning in polynomial time). Stephen Cook and Leonid Levid introduced the concept of NP-complete problems, which we aim to understand in this course. Theoretical com- puter science In theoretical computer science, we consider different problems and see how much time or space we would need to solve them. However, we don’t stop there, and we try as well to make link between problems: see if one problem is easier than another, if randomness or quantum computers would help, if solving one implied another, and so on. For instance, we want to understand efficiently solvable problem (P), efficiently verifiable problems (NP) and undecidable problems. 13 Theory of computation CHAPTER 2. INTRODUCTION 14 Chapter 3 Finite automatons 3.1 Deterministic finite automaton Fixed memory We will want to solve some problems using a fixed number of memory. This means that we scan the input from left to right and, for every symbol seen, we can only change the memory using a sate based on the current state and the current symbol. Definition: DFA A deterministic finite automaton (DFA) M is a 5-tuple (Q, Σ, δ, q0, F ), where: 1. Q is a finite set called the states (which must not depend on the length of the input). 2. Σ is a finite set called the alphabet. 3. δ : Q × Σ ↦→ Q is the transition function. 4. q0 ∈ Q is the start state. 5. F ⊆ Q is the set of accepting states (where we allow F = ø). Note that δ(q, σ) encodes the state we go to from q when reading σ ∈ Σ. We extend this definition such that, for a string s, we use δ(q, s) to denote the state obtained by reading all of s starting in state q. Observation We notice that, for the function δ to be complete, there must be one and exactly one new state for any state and any character. Remark Note that this may seem a bit abstract, but this will become very clear using the following examples and their diagrams. This abstrac- tion will allow us to reason on it. Language of a machine We let A to be the set of all string that the machine M accepts. We call A the language of the machine M and write L(M ) = A. We say that M recognises or accepts A. For instance, if the machine accepts no string, it recognises the empty language ø. Definition: Empty string The empty string, meaning the string which does not contain any character, is written ε. Example 1: Par- ity Let’s say we receive as input a string s made of symbols M and W . We want to output yes if M appears in s an even number of times, and no otherwise. We want to solve this problem using 1-bit of memory and, to do so, we use a DFA. The alphabet is Σ = {M, W } (given by the problem description), and we have some states (the circles), transition functions (the arrows), a starting state (the additional arrow pointing at even) and some accepting state (circles which are circled, “even” in this case). We can make a diagram of this: 15 Theory of computation CHAPTER 3. FINITE AUTOMATONS We can draw the evolution of the state with the following table, if we for instance read MWWMWMM: Read Initial M W W M W M M State E O O O E E O E Since we ended up in the accepting state, we return true. Remark Note that our DFA accepts the empty string ε. Example 2 Let’s consider the following DFA: We want to know what it does. To do so, it is a good idea to try it on a few strings. For instance, 000 and 010 would not be accepted, but 011 would be. Spending a bit more time on it, we can see that when we read a 0, we always go to q1. This means that it accepts all strings that end with a 1. Example 3 Let’s consider the following DFA: It is very close to the last example. This one accepts all string ending with a 0, and the empty string ε. Example 4 Let’s consider the following DFA, which accepts strings with at least a 1 and that end in an even number of 0s: We can describe formally with M = (Q, Σ, δ, q1, F ) where Q = {q1, q2, q3}, q1 is the start state, Σ = {0, 1}, F = {q2} and δ is described as: δ 0 1 q1 q1 q2 q2 q3 q2 q3 q2 q2 Then, we say that M recognises the language: L(M ) = {w | w contains at least one 1 and an even number of 0s follow the last 1} Note that this formal definition and the diagram above are completely equivalent. 16 3.1. DETERMINISTIC FINITE AUTOMATON Notes by Joachim Favre Proving correct- ness To prove the correctness of a DFA, that it indeed recognises a given set, we will use induction. Example 1 Let us consider again the following DFA: We want to prove that M accepts exactly L where: L = {w | count(w, a) is even} where count(s, c) returns the number of times c appears in s. The following proof will be very pedantic, but it is a good way to understand how to make really formal proofs. Proof We want to show that, for all strings w, δ(qo, w) = q0 (M accepts w) if and only if count(w, a) is even. Let’s begin with the base case, w = ε. We know that δ(q0, ε) = q0. However, for the empty string, count(ε, a) = 0, which is indeed even. Now, let’s do the inductive case. We assume that our claim holds for an arbitrary string x, and we want to show that the claim works for w = x.σ where σ is a symbol (a or b) and the dot represents appending a character to a string. We have four different cases for δ(q0, x), which can be either equal to q0 or q1, and σ can either be a or b. We will only consider the case where δ(q0, x) = q0 and σ = b, the others are similar. By the inductive hypothesis, count(x, a) is even. By definition of δ, and since δ(q0, x) = q0 (we just took this hypothesis when selecting one of the four cases): δ(q0, x.b) = δ(δ(q0, x), b) = δ(q0, b) = q0 However, since adding a b does not change the number of a’s a string contains, count(x.b, a) = count(x, a), which is even by inductive hypothesis. This finishes our proof. □ Example 2 Let’s consider the following DFA: We want to show that: L(M ) = {w | w contains at least two a’s} Wrong proof We want to prove that, for all strings w, δ(q0, w) = q2 if and only if count(w, a) is at least 2. This proof will fail. Let’s start with the base case, w = ε. By definition, δ(q0, ε) = q0. Also, count(w, a) = 0 < 2. Both sides of our equivalence are false, showing that base case holds. Let’s now make the inductive step, and let σ ∈ {a, b}. Let’s consider the case where δ(q0, x) = q0 and σ = a. In this case, by induction hypothesis, count(x, a) < 2. However, it would not break our 17 Theory of computation CHAPTER 3. FINITE AUTOMATONS hypothesises that count(x, a) = 1 (this would never happen in practice since the count must be 0 in q0, but our hypotheses do not catch this). In this case, we would have count(x.a, a) = 2 but δ(q0, x.a) = δ(q0, a) = q1, meaning that our claim does not hold. Better proof Let’s make a better proof. To do so, we need to prove something stronger than our claim, something that tells us more about the automata. We will thus try to prove: δ(q0, w) =    q0 if count(w, a) = 0 q1 if count(w, a) = 1 q2 if count(w, a) ≥ 2 The base case holds rather trivially. Concerning the inductive step, we have even more cases to consider. However, doing it is not too hard and completely solves the problem we have just had. Remark We can always prove the correctness of any automata using strong induction. Let’s see the general recipe. Let’s say that we are given an arbitrary language L described by a mathematical constraint, and a DFA M = ({q0, . . . , qn}, Σ, δ, q0, F ). To prove L(M ) = L, we need to start by finding a precise description of the sets Ti of strings that make the machine go to state qi, for all i = 0, . . . , n. For instance, in our example above, T0 was the set of strings such that count(w, a) = 0, and similarly for T1 and T2. Naturally, the language L should match the sets corresponding to the accepting states. Then, we need to prove by induction that, for any string w, δ(q0, w) = qi if w is in set Ti; for all i = 0, . . . , n. Monday 27 th February 2023 — Lecture 2 : Killing cats Definition: Regu- lar language A ⊆ Σ∗ is named a regular language if there exists at least a DFA M such that A = L(M ) (meaning that it is the language of some DFA). Example For instance, the following language is a regular language, as we have already seen a DFA for it: L = {w ∈ {0, 1} ∗ | w contains an even number of 1’s } Formally, we would need to prove the correctness of the DFA. Definition: Com- plement The complement of some language L is defined as: L = {w ∈ Σ ∗ | w ̸∈ L} Theorem: Com- plement Let L be some regular language and M = (Q, Σ, δ, q0, F ) be a machine recognising it. Then, the complement of L, L, is regular and is the language of the following machine: M ′ = (Q, Σ, δ, q0, F = Q \\ F ) Remark We just swap the accepting state and the non-accepting state. This naturally means that strings that were accepted are no longer, and inversely. Definition: Union The union of two languages L1 and L2 is: L1 ∪ L2 = {w ∈ Σ ∗ | w ∈ L1 or w ∈ L2} 18 3.1. DETERMINISTIC FINITE AUTOMATON Notes by Joachim Favre Theorem: Union Let L1 and L2 be the languages of machines M1 = (Q1, Σ, δ1, q1, F1) and M2 = (Q2, Σ, δ2, q2, F2), respectively. Then, the union of L1 and L2, L1 ∪ L2 is regular, and it is accepted by the machine M = (Q, Σ, δ, q0, F ), where: Q = Q1 × Q2, δ((r1, r2), a) = (δ1(r1, a), δ2(r2, a)) q0 = (q1, q2), F = {(r1, r2) | r1 ∈ F1 or r2 ∈ F2} Intuition We are basically simulating the two machines at the same time, keeping track of the two states as a tuple. We then consider that we end up in an accepting state if either machine ended up in one. Example Let us consider the following two machines: Then, their union is: Definition: Inter- section The intersection of two languages L1 and L2 is: L1 ∩ L2 = {w ∈ Σ ∗ | w ∈ L1 and w ∈ L2} Theorem: Inter- section Let L1 and L2 be the languages of machines M1 = (Q1, Σ, δ1, q1, F1) and M2 = (Q2, Σ, δ2, q2, F2), respectively. Then, the intersection of L1 and L2, L1 ∩ L2 is regular, and it is accepted by the machine M = (Q, Σ, δ, q0, F ), where: Q = Q1 × Q2, δ((r1, r2), a) = (δ1(r1, a), δ2(r2, a)) q0 = (q1, q2), F = {(r1, r2) | r1 ∈ F1 and r2 ∈ F2} Intuition This is the exact same construction as for intersection, except that we consider the accepting states to be the ones where both DFA ended up in one. Definition: Con- catenation The concatenation of two languages L1 and L2 is: L1 ◦ L2 = {w ∈ Σ ∗ | w = w1.w2, w1 ∈ L1 and w2 ∈ L2} Observation It is very hard to know if concatenating two regular languages always yield a regular language. The main problem is that we do not know when to switch from the first machine to the second one: a string can be obtained through concatenation in many ways (101 = 1 ◦ 01 = 10 ◦ 1 = 101 ◦ ε). 19 Theory of computation CHAPTER 3. FINITE AUTOMATONS We will see that the concatenation of two languages is indeed regular but, for this, we need to do some non-determinisim. 3.2 Non-deterministic finite automaton Definition: Power set Let Q be some set. We note 2 Q the power set of Q, meaning the set of all the subsets of Q. Example For instance, let us consider Q = {q1, q2}. Then: 2 Q = {ø, {q1}, {q2}, {q1, q2}} Definition: NFA A non-deterministic finite automaton (NFA) M is a 5-tuple (Q, Σ, δ, q0, F ), where: • Q is a finite set called the states. • Σ is a finite set called the alphabet. • δ : Q × (Σ ∪ {ε}) ↦→ 2 Q is the transition function. • q0 ∈ Q is the start state. • F ⊆ Q is the set of accept states, where we allow F = ø. A NFA has multiple ways to compute the final state for some string. It accepts the word if there exists a set of choices, that make the word to be accepted. In other words, if we run a parallel computer that follows all possibilities in parallel, we consider the word to be accepted if any of them are in an accepting state at the end. Comparison with DFA A NFA has two main difference to DFA. First, it has the ability to transition to more than one state on a given symbol, or no symbol at all: the codomain of δ allows any subset of Q, meaning that it could map to ø or {q1, q2}. Note that if at any point we reach a state where we cannot transition out (because δ(σ, qi) = ø), then the path ends and does not continue further. This path is not considered accepted even though it died on an accepting state. Second, it has the possibility to take a step in-between reading symbols, thanks to ε transition. In other words, if δ(ε, qi) = {qj}, then the machine can decide to transition form qi to qj even without reading any character. Example 1 Let’s consider the state diagram of the following NFA: As we can see, there are indeed some states which can transition to more than one state on a given symbol, some which have no transition, and some which have an ε-transition. A good way to see the computation of an NFA is a tree where any level represents all the possible’ states reachable at that point. In this case, if we read input 010110, then we get: 20 3.2. NON-DETERMINISTIC FINITE AUTOMATON Notes by Joachim Favre Note that, as mentioned before, a thread which cannot take transition is killed (the fact that the professor represented the different parallel threads as cats makes this really violent). Also, we can see that the ε transitions can be taken in-between reading symbols, this was mentioned above as well. Since, in this example, we have some threads which end up in an accepting state at the end of the string, then it is accepted. Observation Note that, to show an input is accepted, we don’t need to draw the whole tree, only the branch going to an accepting state. This is named guess and verify. Remark In fact, we can convince ourselves that this NFA recognises the following language: L = {w ∈ {0, 1}∗ | w contains 11 or 101 as a substring } Formal defini- tion The formal definition of our machine is M = (Q, Σ, δ, q1, F ), where: Q = {q1, q2, q3, q4}, Σ = {0, 1}, F = {q4} Also, the function δ is defined as: δ 0 1 ε q1 {q1} {q1, q2} ø q2 q3 ø {q3} q3 ø {q4} ø q4 {q4} {q4} ø Just as for DFA, this formal definition and the state diagram above are completely equivalent. Example 2 Let’s consider the following NFA: We notice that, for instance, “1000” is not accepted since the only thread which reaches q4 dies before the end of the string. We can convince ourselves that it recognises the following language: L = {w ∈ {0, 1}∗ | w contains a 1 in the third position from the end } 21 Theory of computation CHAPTER 3. FINITE AUTOMATONS Example 3 Let us now consider an NFA over a unary language: The beginning with the two ε-transitions, the only non-deterministic part of the machine, allows to have a non-deterministic choice of starting state. This machine is the union of two regular languages: w ∈ {0} ∗ has a length which is a multiple of 2, and w ∈ {0}∗ has a length which is a multiple of 3. This construction is more efficient in size, and it gives: L = {w ∈ {0}∗ | w has a length which is multiple of 2 or 3 } Remark An NFA may be much smaller than its deterministic counterpart, or its functioning may be easier to understand. In fact, they seem really more powerful than DFA, but this is a wrong intuition as we will see with the following theorem, for which we will need some lemma beforehand. Theorem Every non-deterministic finite automaton has an equivalent deterministic finite automaton. Proof We will only prove the case without ε-transitions since it slightly complicates the initial states. However, the proof with them is very similar, and we will show in the second series of exercises that any NFA with ε-transitions can be translated to any NFA without such transitions. Let N = (QN , Σ, δN , q0, Fn) be an arbitrary NFA. We want to construct a DFA M such that: L(N ) = L(M ) The idea is to consider that a state of the DFA represents the set of states from the NFA which could be reached by one of the threads. In other words, the transition function maps a set of states into another set of all the possible states the threads could reach in one step. Mathematically, we let M = (2 Q, Σ, ̃δ, {q0}, ̃F ), where: ̃δ(A, a) = ⋃ q∈A δ(q, a) Also, we let ̃F to contain all the sets of states which contain an accepting state: ̃F = {A ∈ 2 Q | A ∩ F ̸= ø } We now want to show that M ’s state after x ∈ Σ ∗ is ̃δ({q0}, x) = A if and only if A is N ’s possible states after reading x. We will prove this by induction. Let’s begin with the base case, taking x = ε. We see that ̃δ({q0}, ε) = {q0} by definition of the starting state, which is indeed 22 3.2. NON-DETERMINISTIC FINITE AUTOMATON Notes by Joachim Favre N ’s possible states after ε. Note that this is where appears our hy- pothesis that our machine does not have an ε-transition, since, else, we may not have the equality hereinabove and it thus complicates the proof. Let us now consider the inductive step. As usual, we assume that the claim holds for x, and we want to prove it for x.a. We see that: ̃δ({q0}, x.a) = ̃δ(̃δ({q0}, x), a) déf = ⋃ q∈̃δ({q0},x) δ(q, a) where ̃δ({q0}, x) is N ’s possible states after reading all the characters of x, as usual. However, we are taking the union of all the states we can reach from those those possible states, keeping our inductive property. In other words, since we indeed had all the possible states of N for x, then we still have all the possible states of N after x.a, as required. □ Example Let’s consider this construction for the following NFA: We can construct a new DFA which considers all the 8 subsets of {p, q, r} as states, and we consider an accepting state to be any set containing r: For instance, if we are in any of the state q or r and we read a 1, we can go to any of p, q and r. This indeed means that δ({p, q}, 1) = {p, q, r}. We can see that the this DFA can never reach some of its states, such as ø. Corollary A language is regular if and only if some NFA recognises it. Theorem: Con- catenation Let L1 and L2 be regular languages. Then, the concatenation of L1 and L2, L1 ◦ L2 = L, is regular. Proof Concatenation is very easy with non-determinism. Indeed, we just need to link the accepting states of the first machine to the initial state of the second machine using ε-transition. In other words, we allow to switch to the second machine as soon as we recognise a word, but also to stay on the first one. 23 Theory of computation CHAPTER 3. FINITE AUTOMATONS However, by the corollary above, since we have a NFA which lan- guage is the concatenation of L1 and L2 is indeed regular. □ Monday 6 th March 2023 — Lecture 3 : ”Pigeon collisions” 3.3 Non-regular languages Existence of non-regular languages We have seen that many languages are regular: using a finite automaton, we can verify a number has a given substring, check if it is divisible modulo a given number, skip some prefix and suffix, do complements, unions, intersections, and so on. We now wonder if all languages are regular. In fact, sadly, no. Let us consider the following language: B = {0 n1 n | n ≥ 0} = {ε, 01, 0011, 000111, . . .} The problem is that we need some kind of memory depending on the size of the input: we need to recall how many zeroes we had when we are counting the ones. Proof Let us prove that B is indeed not regular. Let’s suppose for contradiction that B is recognised by a DFA with p states. Note that p has to be finite. Now, let us consider the countable infinite set {0 1, 0 2, . . .} . By the pigeon hole principle, since there is a finite number of states p but an infinite number of 0k, there must exist i, j such that 0i and 0j end up in the same state. We have a pigeon collision. However, this means that if 0i1i is accepted, then 0j1i is also accepted. However, 0j1i should not be accepted, which is our contradiction. □ Remark Note that, to show a language is not regular, we cannot just say “it needs memory depending on the size of the input” blindly. Let us consider the following language: D = {w | w has an equal number of occurrences of 01 and 10 as substrings} In fact, we can show that it is equivalent to: D = {w | w begins and ends with the same character, or length(w) ≤ 1} Thus, D is regular. 24 3.3. NON-REGULAR LANGUAGES Notes by Joachim Favre Pumping lemma If A is a regular language, then there is a number p (named the pumping length) such that, for every string s in A of length at least p, there exists a division of s into three pieces, s = xyz, such that: 1. For any i ≥ 0, xyiz ∈ A. 2. |y| ≥ 1. 3. |xy| ≤ p. Intuition It is much better to remember the proof of this lemma, instead of the lemma itself. In fact, it just generalises the argument we had before. Let p = |Q|. By the pigeon hole principle, if we have a string of length at least p, we need to have a loop: let y be a string which loops from a state qi to the same state qi. Then, we also need x to be a string bringing from the initial state to qi, and z to be a string bringing from qi to an accepting state. We can then go from the initial step to qi, loop any number of times we want (even 0 times) and go to an accepting state. |xy| ≤ p just means that the state collision (which leads to the loop) has to take place before p. Proof Let M be an arbitrary DFA accepting A, and let p = |Q| be the number of states in M . Also, let s be any arbitrary string of length |s| ≥ p. By the pigeonhole principle, since we have p states and we visit p + 1 states when reading a string of length p, this means that we must always have some collision for any string which length is less than or equal to p. Thus, when reading the p first characters of s, there must be a loop: there must be some x, y, z such that |xy| ≤ p (the loop must happen in the first p characters of s), |y| ≥ 1 (because of the loop) and: δ(q0, x) = q, δ(q0, xy) = q, δ(q, z) = r ∈ F In other words, x takes us to some state q; then, y loops us back to this same state; and finally z brings us to an accepted state. However, this definitely means that: δ(q0, xyiz) = r, ∀i ≥ 0 □ Example 1 Let us consider the following language: F = { ww | w ∈ {0, 1} ∗} In other words, we only want words which left part is equal to its right part. Let’s suppose for contradiction that this language is regular. By the pumping lemma, it must have some pumping length p. Now, any long string can be pumped. Let’s pick s = 0p1p0p1p ∈ F (note that the choice of which string to pick is the whole game when doing such proofs). By the pumping lemma, we know that there exists some x, y, z such that: s = xyz, |xy| ≤ p, |y| ≥ 1, xyiz ∈ F, ∀i ≥ 0 25 Theory of computation CHAPTER 3. FINITE AUTOMATONS However, since |xy| ≤ p, this necessarily means that xy = 0 j for some j by construc- tion. Moreover, since |y| ≥ 1, we get that y = 0k for some k ≥ 1. According to the pumping lemma, we must have xy2z ∈ F . However: xy2z = 0p+k1 p0 p1p ̸∈ F since the left part of this new string does not match the right one. This is our contradiction. Example 2 Let us consider the following language: E = {0m1n | m > n} We want to show that E is not regular. Let us suppose for contradiction that E is regular, with p as pumping length. Let us consider the following string: s = 0p+11p ∈ E Since we know that |xy| ≤ p and |y| ≥ 1, we necessarily have that y = 0k for some k ≥ 1. However, we see that: xy0z = xz = 0p+1−k1 p ̸∈ E since p + 1 − k ≤ p. By the pumping lemma, we should have xz ∈ E, this is our contradiction. Example 3 Let us consider the following language: C = {w | w has an equal number of 0s and 1s} We again want to show that it is not regular. Thus, let’s suppose for contradiction that C is regular, and that p is its pumping length. Let us consider the following string: s = 0p1p ∈ C Since we know that |xy| ≤ p and |y| ≥ 1, we necessarily have that y = 0k for some k ≥ 1. Moreover: xy2z = 0p+k1 p ̸∈ C However, since the pumping lemma tells us that xy2z ∈ C, we got our contradiction. Other proof We can use an easier approach than the pumping lemma. Let us consider again a language we have proven not to be regular at the start of this lecture: B = {0 n1 n | n ≥ 0} = {ε, 01, 0011, 000111, . . .} However, we can see that: B = C ∩ {0 m1 n | m, n ≥ 0} Let us now suppose for contradiction that C is regular. However, we know that {0m1n | m, n ≥ 0} is a regular language, and the intersection of two regular language is also regular. This thus means that B is regular, which is a contradiction. This shows that the pumping lemma, though very powerful, is not the always the only way to go. 26 Monday 13 th March 2023 — Lecture 4 : Turing , Chapter 4 Turing machines 4.1 Formal definition Introduction So far, it feels like we miss memory: we would need some kind of memory depending on the size of the input (while keeping a fixed-size program). This is where Turing machines come in. The idea is that we have a tape on which we can write and read things. Thus, we have like a finite automaton where we can also take into account what is written on the tape. We still have a finite number of states (the program is finite), but the tape is infinite (allowing for different inputs to use different size of memory). Observation We only need a single tape since we can reserve some part of the memory for the input and the output. It is possible to show that having multiple tapes or a single one is completely equivalent. Capabilities Even though the tape is infinite, the Turing Machine can only see a specific symbol, where its current head is. At any given step, it acts depending on what is written on the tape under the head and the current state. Just as for DFA, it goes to some state. However, moreover, it can move the tape right or left, and write any symbol on it (including a new symbol, ⊔, representing an empty cell). As soon as it reaches the accept or reject state, the machine stops immediately, unlike DFA. Because of that we can have a single accept and reject states (one of each), since it would not be useful to have more. Moreover, this means that it may never halt: it may never reach an accept or reject state. Example Let us consider the following diagram: 27 Theory of computation CHAPTER 4. TURING MACHINES Let’s say the head begins at the first character of “1001”. We start at q0 and read a 1, so we transition to state q1, empty the current cell, and move the tape one to the right. We can continue this until the accepting state. We can in fact convince ourselves that this Turing machine accepts even length binary palindromes. Remark This is a very inefficient algorithm to find palindromes. However, in this part of the course, we do not care about running time. Definition: Tur- ing Machine A Turing Machine is a 7-tuple (Q, Σ, Γ, δ, qo, qaccept, qreject) (with Γ and qreject being the new elements with respect to DFA), where: 1. Q is the set of states. 2. Σ is the input alphabet, not containing the blank symbol ⊔. 3. Γ is the tape alphabet, where Σ ∪ {⊔} ⊆ Γ (and it potentially contains more characters). 4. δ : Q × Γ ↦→ Q × Γ × {L, R} is the transition function. 5. q0 ∈ Q is the start state. 6. qaccept ∈ Q is the accept state. 7. qreject ∈ Q is the reject state, where qreject ̸= qaccept. Definition: TM configuration Let’s consider the Turing Machine (Q, Σ, Γ, δ, q0, qaccept, qreject). We still need to define how it computes. We write uqv where u, v ∈ Γ ∗ and q ∈ Q for the configuration where: 1. The current state is q. 2. The current tape content is uv. 3. The curent head location is the first symbol of v. 4. The cells whose content are unspecified are blank, they contain ⊔. Given a configuration uaqibv where a, b ∈ Γ, u, v ∈ Γ∗ and qi ∈ Q, we move to: { uqjacv, if δ(qib) = (qj, c, L) uacqjv, if δ(qi, b) = (qj, c, R) We start at some configuration C1 = q0w for some w ∈ Σ∗. We then obtain new configurations C2, C3, . . . by valid transitions. We accept the word w if the machine halts and if the state qaccept is reached, but we reject it if it halts and the state qreject is reached. Naturally, the program may not terminate. We can even show that some loops may happen even though no configurations are repeated (for instance if the machine writes a 1 to the current cell and moves to the right). 28 4.2. DECIDABLE AND UNDECIDABLE LANGUAGES Notes by Joachim Favre Example For instance, a Turing Machine with configuration 1011q701111 would look like: Example We want to make a Turing Machine which recognises the following language: L = {w | w has an equal number of 0s and 1s} We have an input alphabet Σ = {0, 1}. We consider a tape alphabet which has an extra character X, which represents a crossed off letter: Γ = {0, 1, ⊔, X} The idea is to scan the input from left to right. Whenever we encounter an uncrossed 0 or 1, we replace it by a cross, and proceed to the right to find a corresponding 1 or 0 that we cross. We then go back at the start of the input, and start again. If we end up crossing out every characters, we accept it, but if we can’t find a corresponding 1 or 0 to a 0 or 1, we reject it. Note that we are doing something slightly more intelligent here: when we are at the left of the string and scan for the first non-X character, we also replace all X characters by ⊔, in order to simplify the detection when we finished considering the whole word. 4.2 Decidable and undecidable languages Definition: Turing- Recognisable A language L is Turing-Recognisable (or Recognisable) if there exists a Turing Machine M which recognises L. In other words, all w ∈ L must be accepted by M and all w ̸∈ L do not make M stop or M rejects w. Definition: Turing-Decidable A language L is Turing-Decidable (or Decidable) if there exists a Turing Machine M which decides L. In other words, all w ∈ L must be accepted by M and all w ̸∈ L must be rejected by M . Observation This second definition is stronger: if a machine decides a language, then it recognises it. However, the converse may not be true. The machine may not stop on some words w ̸∈ L. 29 Theory of computation CHAPTER 4. TURING MACHINES Church-Turing thesis In 1936, two scientists gave different definitions for the concept of algorithms. Church defined it using λ-calculus, in an intuitive way. Turing defined it using his machines. However, we can show that the two definitions are equivalent. This means that anything we can write in programming languages (such as C, Java, and so on) or with quantum computers can also be executed on Turing Machines; and conversely. Thus, when we want to show that a language is decidable or recognisable, we do not necessarily mean to make a formal Turing Machine, a program is sufficient. Monday 20 th March 2023 — Lecture 5 : Halting problem Definition: En- coding Let S be any object. We write its binary encoding ⟨S⟩. Example 1 A Turing Machine can thus, for instance, decide whether a graph is connected, or find a shortest path. To give a graph G as an input to a Turing Machine, we just give ⟨G⟩. Example 2 We want to make a Turing machine which checks if a DFA is empty, if it rejects every word. The first idea may be to try all possibilities s ∈ {ε, 0, 1, 00, . . .} and see if the DFA accepts it. The problem with this approach is that, if the DFA accepts nothing, then we will never halt. A second, better idea, is to check if there exists a set of transitions bringing from the initial state to an accepting state. This can be done using BFS, considering the DFA as a graph; which always halts. Example 3 We want to make a Turing Machine which decides whether two DFAs recognise the same language. We notice that L(D) = L(D′) if and only if L(D) ⊕ L(D′) = ø, where ⊕ is the symmetric difference of state (an element is in the symmetric difference if it is in one set xor the other). Moreover, we can see that: L(D) ⊕ L(D′) = (L(D) ∩ L(D′) ) ∪ (L(D) ∩ L(D′)) We saw that complements, unions and intersections of regular languages were also regular, so L(D) ⊕ L(D′) is regular and there exists a DFA D⊕ accepting it. So, we can make a Turing Machine to create D⊕ from D and D′, and then verify if it is empty using the program made right before. This always halts, so this is perfect. Example 4: Halt- ing problem We now receive a Turing Machine and some input as input, and we want to output whether the machine halts. It is rather easy to show that this problem is recognisable. We can just simulate the Turing machine, and see if it halts. The problem is that it seems very hard to decide. We will develop some more theory and come back on this problem. Definition: Car- dinality of sets Two sets A and B have the same cardinality, written |A| = |B| if there exists a bijective function f : A ↦→ B. Example For instance, we can show that |N| = |Q| (and we did in AICC-1). Definition: Countable set A set A is countable if either it is finite or it has the same cardinality as N. Example As mentioned earlier, Q is countable. 30 4.2. DECIDABLE AND UNDECIDABLE LANGUAGES Notes by Joachim Favre Theorem: R is not countable The set of real numbers is not countable: |R| ̸= |N| Proof The proof can be done using Cantor’s diagonalisation. It was also done in AICC-1, but let’s do it again. Suppose for contradiction that there exists a bijection f : N ↦→ R. We want to find some x ∈ R for which there exists no a ∈ N such that f (a) = x. We construct x by taking a number different from the nth fractional digit of f (n), effectively taking different digits over the diagonal. For instance, if we have the following f : n f (n) 1 3.14159 . . . 2 55.55555 . . . 3 0.12345 . . . 4 0.500000 . . . ... ... Then we could pick 0.3323 . . . There cannot exist some m such that f (m) = x: the m th digit of x is different from the mth digit of f (m), by construction. This thus finishes our proof. □ Theorem There exists some language which is not decidable. Proof We can build a table for Turing Machine: the rows list the Turing Machine (they are countable, so we can indeed make a list of them). The columns represent a binary representation of each machine, which we feed as an input to the machines (this is a bit meta, but this will be useful). Each entry of the table states if the machine accepts the given input, rejects it, or never halts. Now, we construct a language DIAG such that it contains all inputs ⟨Mi⟩ such that the i th machine Mi does not accept this input. In this example, we would have: DIAG = {⟨Mi⟩ | Mi doesn’t accept ⟨Mi⟩} = {⟨M2⟩, ⟨M4⟩, ⟨M6⟩, . . .} We want to show that DIAG is undecidable. Let’s suppose for contradiction that there exists some Turing Machine M which decides DIAG. However, since we made the list of all possible machines, it means that M = Mi for some i ∈ N in the table. Now, we cannot have ⟨Mi⟩ ∈ L(Mi) since it would imply that the machine accepts itself and thus, by definition of DIAG, it would imply that ⟨Mi⟩ ̸∈ DIAG. Similarly, we cannot have ⟨Mi⟩ ̸∈ L(Mi) since it would imply that the machine does not accept itself and thus, by definition of DIAG, it would imply that ⟨Mi⟩ ∈ DIAG. 31 Theory of computation CHAPTER 4. TURING MACHINES We cannot have ⟨Mi⟩ ∈ L(Mi) and ⟨Mi⟩ ̸∈ L(Mi). This is our contradiction. □ Theorem The following language is undecidable: HALT = {⟨M, w⟩ : M is a Turing Machine which halts on w} Proof Let’s assume that there exists some Turing Machine H which decides HALT . We construct a Turing Machine D which decides DIAG, defined in the proof of the previous theorem. On an input ⟨M ⟩, we run H on the input ⟨M, ⟨M ⟩⟩. If H rejects (meaning that M loops on ⟨M ⟩), then we accept. If it accepts (meaning that M halts on ⟨M ⟩), then we run M on the input ⟨M ⟩, and output the opposite of M (accept if it rejects, and inversely). machine D(M): i f H(<M, <M>>) then // meaning that M halts on input <M > return !M(<M>) // output accepted if rejected , and inversely else return accepted // it loops , so we accept We notice that D halts on all inputs: we only run machines we are sure will end. Moreover, D accepts ⟨M ⟩ is equivalent to M not accepting ⟨M ⟩ and thus equivalent to ⟨M ⟩ ∈ DIAG. This is a contradiction, since we showed that DIAG was undecidable. □ Theorem The following language is undecidable: AT M = {⟨M, w⟩ | M is a Turing Machine and accepts w} Proof Let’s assume for contradiction that there exists a decider H for AT M . Again, we construct a decider D for DIAG using H. To do so, we feed M, ⟨M ⟩ to H, and output its opposite value (if it accepts, then we reject, and inversely). machine D(M): return !H(<M, <M>>) As before, D halts on all inputs and D accepting ⟨M ⟩ is equivalent to ⟨M ⟩ ∈ DIAG (we output rejected if and only if the machine accepts itself). This indeed shows that D decides DIAG, which is our contradiction. □ Observation We have seen undecidable languages, but there also exists unrecognisable languages. Theorem A language L is decidable if and only if it is recognisable and its complement is also recognisable. Proof =⇒ The proof is considered trivial and left as an exercise to the reader. Proof ⇐= Let L and L be recognisable, and M1, M2 be their corresponding recognisers. We construct a machine M which runs both M1 and M2 in parallel (we alternate between running 1 step of M1 and one step of M2), 32 4.3. REDUCTIONS Notes by Joachim Favre and stop as soon as M1 or M2 accepts w. If it is M1 which accepted, then we output accept; if it is M2, we output reject. We notice that M must always halt: w ∈ L or w ∈ L, so one of the two machines must halt and output accept. Thus, M decides L. Corollary The language HALT is unrecognisable. Proof We know that HALT is undecidable but recognisable. If HALT was recognisable, then it would mean that HALT would be decidable by our theorem above, which is our contradiction. □ Monday 27th March 2023 — Lecture 6 : Reductions 4.3 Reductions Goal The goal of this section is to make a more abstract-type of proofs, more systematic way, to show that a language is not recognisable or not decidable. The idea is to compare the difficulty of languages. Example Let us consider the language where, given the encoding of some DFA, we output whether it accepts a non-empty language: N EDF A = {⟨D⟩ | L(D) ̸= ø} We can also consider the language such that, given the encoding of some graph and two of its vertices, we output whether there exists a path between those two in the graph: Lpath = {⟨G, v, w⟩ | v, w ∈ V (G) and there exists some path from v to w in G} We notice that solving this second problem allows to solve the first too. If there is a path from the initial state to an accepting state, then the language of the DFA is non-empty. We have done a reduction: we transformed our problem into another one using some function f (⟨D⟩) = ⟨G, v, w⟩, which is easier to solve. Definition: Com- putable function A function f : Σ∗ ↦→ Σ∗ is a computable function if there exists some TM M such that, on every input w, halts with just f (w) on its tape. Definition: Map- ping reducible Language A is mapping reducible to language B, written A ≤m B, if there exists some computable function f : Σ∗ ↦→ Σ∗ named reduction such that, for every w ∈ Σ ∗: w ∈ A ⇐⇒ f (w) ∈ B In other word, f does not need to be bijective, but any element in A must land in B, and any element in A must end in B. Theorem If A ≤m B and B is decidable, then A is decidable. Proof We know that B is decidable, thus let RB be a decider for it. Also, let f be a reduction from A to B. 33 Theory of computation CHAPTER 4. TURING MACHINES We can then define a Turing Machine M such that, it takes any input w, feed f (w) to RB, and output whatever RB outputs. However, by the definition of reduction: w ∈ A ⇐⇒ f (w) ∈ B This is equivalent to: w ̸∈ A ⇐⇒ f (w) ̸∈ B Thus, we indeed have that M is a decider for A: it accepts all words inside A and rejects all inside A. □ Corollary If A ≤m B and A is undecidable, then B is undecidable. Example 1 The following reduction is possible: AT M ≤m HALT And thus, HALT is undecidable. Proof Our goal is to find some reduction f from AT M to HALT . Given some input x = ⟨M, w⟩, our reduction outputs (without running the machine we construct) f (x) = ⟨M ′, w⟩, where M ′ receives some input y, and runs it on M . If M rejects y, M ′ enter an infinite loop; and if M accepts y, then M ′ accepts y. We can indeed show that this f is computable. We constructed M ′ such that it halts on w if and only if M accepts w. Thus, we showed that: ⟨M, w⟩ ∈ AT M ⇐⇒ f (⟨M, w⟩) = ⟨M ′, w⟩ ∈ HALT We have found a reduction, which ends our proof. □ Example 2 We consider the following language REGT M = {⟨N ⟩ | L(N ) is regular} We have that: AT M ≤m REGT M And thus, REGT M is undecidable. Proof Our goal is to map any (M, w) pair to a regular language if M accepts w and to a non-regular language is M rejects or does not halt on w. The idea is to create a Turing Machine M ′ (without simulating it) using some kind of switch. First, if y ∈ {0n1 n | n ∈ N}, the machine will directly accept it. If not, this is where the switch appears: the machine M ′ runs M on w and, depending on the result, accepts y or not. If M accepts w, then M ′ accepts y; meaning that, overall, it accepts any y. If M rejects w, then M ′ rejects y; meaning that overall it only accepts {0 n1 n | n ∈ N}. If M loops on w then, well, 34 4.3. REDUCTIONS Notes by Joachim Favre M ′ loops on w and thus does not accept it; meaning that, overall, it also only accepts {0 n1 n | n ∈ N}. The machine we constructed (but did not run, it is the job of the machine for REGT M to analyse it without halting), M ′, can be represented as: We have thus constructed a Turing Machine which language depends on the acceptance of w by M : if M accepts w, then M ′ has a regular language but, if M halts or rejects w, then M ′ does not have a regular language. Thus: ⟨M, w⟩ ∈ AT M ⇐⇒ f (⟨M, w⟩) = ⟨M ′⟩ ∈ REGT M This is our reduction, finishing our proof. □ Theorem If A ≤m B and B is recognisable, then A is recognisable. Contrapositive If A ≤m B and A is unrecognisable, then B is unrecognisable. Proof We can, as usual, consider the following machine: We know that M accepting w is equivalent to RB accepting f (w), by creation of our machine. Then, this is equivalent to f (w) ∈ B because RB is a recogniser for B. This is finally equivalent to w ∈ A since A ≤m B. M accepts w ⇐⇒ RB accepts f (w) ⇐⇒ f (w) ∈ B ⇐⇒ w ∈ A This thus shows that M is a recogniser for A. □ Example Let us consider the following language, which states whether two Turing Machine decide the same language.: EQT M = {⟨M1, M2⟩ | M1, M2 are Turing Machines such that L(M1) = L(M2)} Then: AT M ≤m EQT M Thus, EQT M is unrecognisable. Proof We define a reduction f such that, given an input x = ⟨M, w⟩, it outputs f (x) = ⟨M1, M2⟩. The goal is that both machines have the same language if and only if ⟨M, w⟩ ∈ AT M . Thus, we construct M1 to ignore its input and just runs M on w, and accepts if M accepts or enter an infinite loop otherwise. M2 rejects any input y. 35 Theory of computation CHAPTER 4. TURING MACHINES We notice that if ⟨M, w⟩ ∈ AT M , then M1 loops on all inputs and thus: L(M1) = ø = L(M2) =⇒ f (⟨M, w⟩) = ⟨M1, M2⟩ ∈ EQT M If ⟨M, w⟩ ̸∈ AT M , then M accepts w and thus M1 accepts every string. This means that: L(M1) = Σ∗ ̸= ø = L(M2) =⇒ ⟨M1, M2⟩ ̸∈ EQT M We have indeed show that f is a reduction, and thus that AT M ≤m EQT M . □ 36 Monday 17 th April 2023 — Lecture 7 : P and NP Chapter 5 Complexity classes 5.1 P complexity class Observation We have already noticed that having a decider for some language is better than a recogniser. Now, amongst deciders, we would like to distinguish which is better. Definition: Run- ning time Let M be a Turing Machine that halts on all inputs (in other words, it is a decider). The running time (or time complexity) of M is the function t : N ↦→ N where: t(n) = max w∈Σ∗ | |w|=n number of steps M takes on w Intuition Intuitively, we consider the worst case for all inputs of size 1, then the worst case for all inputs of size 2, and so on. Asymptotic growth We may do this for two Turing Machine and get two functions, where one is sometimes smaller than the other, and the other is sometimes smaller. We need a way to compare them , so we will only consider asymptotic growth. Definition: Big-O Let f, g : N ↦→ R≥0. We say that f (n) = O(g(n)) if ∃C > 0 and ∃n0 ∈ N such that: ∀n ≥ n0, f (n) ≤ Cg(n) Intuition This means that, asymptotically, f grows at most as fast as g. Example For instance, we have: 5n3 + 1 = O(2n) 5n3 + 1 ̸= O(20n + 5) Definition: Small-o Let f, g : N ↦→ R≥0. We say f (n) = o(g(n)) if, for all c > 0, ∃n0 ∈ N such that: ∀n ≥ n0, f (n) < cg(n) Intuition This is like saying that, asymptotically, f grows strictly slower than g. Example For instance, we have: √n = o(n) f (n) ̸= o(f (n)) Definition: Time complexity class We define complexity classes as: TIME(t(n)) = {L ⊂ Σ∗ | L is decided by a TM with running time O(t(n))} 37 Theory of computation CHAPTER 5. COMPLEXITY CLASSES Property We notice that: TIME(n) ⊆ TIME(n2) ⊆ . . . ⊆ TIME(2n) ⊆ . . . Also, we notice that we have: REGULAR ⊆ TIME(n) Definition: Com- plexity class P P is the class of languages that are decidable in polynomial time on a deterministic Turing machine. In other words: P = ∞⋃ k=1 TIME(nk) Note that this definition is robust to different model of computations. Thus, de- scribing an algorithm working on a Turing Machine, or making one in C++, are equivalent. Example For instance, sorting an array and traversing a graph using BFS are algorithms in the class P. Extended Church-Turing thesis The extended Church-Turing thesis states that P corresponds to the class of problems that are “realistically” solvable in our universe. However, this claim is a little controversial since randomisation and quantum may help solving problems faster than anything doable on a Turing Machine. However, we don’t know if this is really true, we just found some algorithms running faster on a quantum computer, though we did not prove that regular computers could not have any faster algorithms. 5.2 NP complexity class SAT-verify and SAT We want to define some language SAT-verify and some harder language SAT. For it, we need to first define some other objects. CNF A CNF (conjunctive normal form) is an expression with sub- expression composed of OR and NOT, which are composed using AND. For instance: ϕ1 = (x ∨ y ∨ z0) ∧ (x ∨ y ∨ z1) ∧ (x ∨ y ∨ z2) ∧ (x ∨ y ∨ z3) ϕ2 = x1 ∧ (x1 ∨ x2) ∧ (x1 ∨ x2 ∨ x3) ∧ (x1 ∨ x2 ∨ x3 ∨ x4) ϕ3 = x1 ∧ (x1 ∨ x2) ∧ (x1 ∨ x2) Satisfiable sentence A satisfying assignment is a set of variables which make the variable true. A sentence is satisfiable if there exists at least one satisfying assignments. For instance, ϕ1 has 32 satisfying assignments, ϕ2 has only one and ϕ3 has zero. SAT-verify We define the following language: SAT-verify = {⟨ϕ, C⟩ | C is a satisfying assignment of ϕ} It is clearly in P since we can just substitute for the literals according to C, and see if the result is true. 38 5.2. NP COMPLEXITY CLASS Notes by Joachim Favre SAT We now define the following, harder, language: SAT = {⟨ϕ⟩ | ϕ is satisfiable} = {⟨ϕ⟩ | ∃C such that ⟨ϕ, C⟩ ∈ SAT-verify} We notice that we can iterate over all possible assignments C, and see if ⟨ϕ, C⟩ ∈ SAT-verify. However, this runs in exponential time. There are lots of much more complicated algorithms which are a bit better, but they all run in expected exponential time. GI-verify and GI We want to define some languages GI-verify and GI. We will need to define some objects first. Graph iso- morphism A graph isomorphism is a bijection f : V (G1) ↦→ V (G2) which preserves adjacency: {u, v} ∈ E(G1) ⇐⇒ {f (u), f (v)} ∈ E(G2) Two graphs are isomorphic if they have at least one graph ismorph- ism. In other words, they are isomorphic if we can just re-label the vertices, to get one another. GI-verify We define the following language: GI-verify = {⟨G1, G2, C⟩ | C : V (G1) ↦→ V (G2) is a graph ismorophism} We notice that GI-verify is P. Indeed, we only need to iterate over the edges (which is quadratic in the number of vertices) and check that: {u, v} ∈ E(G1) ⇐⇒ {C(u), C(v)} ∈ E(G2) GI Now, let’s consider the following language: GI = {⟨G1, G2⟩ | G1 and G2 are isomorphic} = {⟨G1, G2⟩ | ∃C such that ⟨G1, G2, C⟩ ∈ GI-verify} Again, this problem is very hard to solve, and we don’t have any solution in polynomial time. However, the naive way, to try all the possible isomorphisms, is not the best asymptotic one known so far. The best known runs in quasi-polynomial time, nO(log(n)), but this is still not polynomial. INDSET-verify and INDSET Independent set In a graph, an independent set is a subset S ⊆ V (G) such that no two vertices in S are adjacent in G. For instance, in the following graph, {1, 3, 5}, {6} and ø are inde- pedent sets. INDSET-verify Let us consider the following language: INDSET-verify = {⟨G, k, C⟩ | C is an independent set of size k in G} 39 Theory of computation CHAPTER 5. COMPLEXITY CLASSES This can be checked in polynomial time. Indeed, we can first check that |C| = k. Then, we loop for each u, v ∈ C, and we verify that: {u, v} ̸∈ E(G) INDSET Let us now define the following, more complicated language: INDSET = {⟨G, k⟩ | G has an independent set of size k} = {⟨G, k⟩ | ∃C such that ⟨G, k, C⟩ ∈ INDSET-verify} Again, the naive solution of looping over all possible inputs of size k takes exponential time: (n k) = n O(k) (which is not polynomial since k is not constant; we might have k = n 2 ). As for the two other problems, we do not know any solution solving this in polynomial time. Definition: Veri- fier A verifier for a language L is a TM M such that for each x ∈ Σ ∗: x ∈ L =⇒ ∃C such that M accepts ⟨x, C⟩ x ̸∈ L =⇒ ∀C, M rejects ⟨x, C⟩ We call those C a certificate (or witness). Definition: Poly- nomial time verifier A polynomial time verifier (poly-time verifier) is a verifier which has a running time in x on any ⟨x, C⟩ is polynomial in x. Remark This allows to know that |C| is polynomial in |x|, since we will need to read the whole certificate and it must thus not be longer than the time we are allowed to take. Observation We can turn any verifiable language to a decidable language, by iterating over all the possible inputs. However, for a polynomial verifiable language, this gives a decider running in exponential time. Definition: NP NP is the class of languages that have polynomial-time verifiers. Observations We trivially notice that: P ⊆ NP Indeed, we can make a verifier which just uses the decider, and ignores the given certificate. It indeed runs in polynomial time. Now, we don’t know if P = NP. This is actually one of the 7 millennium problems, showing its difficulty. However, most researchers actually think that P ̸= NP, since, otherwise, it would for instance be very easy to prove theorems (since we can verify that a proof is correct in polynomial time). Monday 24 th April 2023 — Lecture 8 : Turning satisfying assignments to graphs Proposition: SAT SAT is in NP. Proof We construct a verifier for SAT: given some input ⟨ϕ, C⟩, we interpret C as a truth assignment to the variables of ϕ, and check if our sentence indeed reduces to true. We can thus indeed write our language as: SAT = {⟨ϕ⟩ | ∃C such that the above verifier accepts ⟨ϕ, C⟩} Proposition: GI GI is in NP. 40 5.2. NP COMPLEXITY CLASS Notes by Joachim Favre Proof We construct a verifier for GI: given some input ⟨G1, G2, C⟩, we interpret C as function V (G1) ↦→ V (G2) and check that it is both a bijection and preserves adjacency. This is a poly-time verifier, indeed showing this is in NP. Proposition: INDSET INDSET is in NP. Proof We construct a verifier for the language INDSET: on input ⟨G, k, C⟩, we interpret C as a subset C ⊆ V (G), check that |C| = k and check that, for all u, v ∈ C, we have {u, v} ̸∈ E(G). Since this verifier runs in polynomial time, we have indeed shown INDSET is in NP. Definition: Non- deterministic Turing Machines In a regular Turing machine, the transition function is δ : Q × Γ ↦→ Q × Γ × {L, R}. In a Nondeterministic Turing Machine (NTM), we instead let it to be: δ : Q × Γ ↦→ P(Q × Γ × {L, R}) We thus allow several transitions for any given state and tape symbol, just as for NFAs. Observation Those are not really computable efficiently, we would require an exponential amount of parallel threads. Multithreading Note that NTMs are weaker than multithreaded computers: threads cannot communicate between each other. This is more some kind of “quantum parallel worlds”. Definition: Non- deterministic decider A nondeterministic decider for language L is an NTM N such that for each x ∈ Σ ∗, every computation of N on x halts. Moreover, if x ∈ L, then some threads of N on x accepts and, if x ̸∈ L, then every computation of N on x rejects. Definition: Poly- nomial time NTM An NTM is a polynomial time NTM if the running time of its longest computation on x is polynomial in |x|. Theorem Let L ⊆ Σ ∗ be an arbitrary language. L has a nondeterministic poly-time decider if and only if L has a poly-time verifier. Proof =⇒ We know by hypothesis that there exists some nondeterministic polynomial-time decider N for L We construct a verifier V for L. It interprets the certificate as a set of choices for which thread to choose. In other words, on input ⟨x, C⟩, it simulates N (x) with nondeterministic choices given by C. We know that there exists a path in our decision tree which has a polynomial length by definition of polynomial time NTM. Thus, we know this path can be encoded by a certificate with polynomial length. Proof ⇐= We know by hypothesis that there exists a verifier V for L. We construct a nondeterministic poly-timer decider. Using non- determinism, we can try all certificates in parallel. In other words, at each step, we split into three threads: one which appends a 0 to the current certificate, one that appends a 1, and one that uses the current certificate (which runs V on ⟨x, C⟩). If there exists a certificate which works, we will find it in polynomial time. This gives us an algorithm which runs in polynomial time for each thread, which is thus a polynomial-time NTM. □ 41 Theory of computation CHAPTER 5. COMPLEXITY CLASSES Definition: NTIME com- plexity class We define the NTIME complexity class as: NTIME(t(n)) = {L | L has a nondeterministic O(t(n)) time decider} Equivalent defini- tion: NP Class NP can be equivalently defined as: NP = ∞⋃ k=1 NTIME(nk) Remark This explains why NP is called nondeterministic P: a language in NP has a nondeterministic polynomial-time decider. 5.3 Polynomial-time reductions Definition: Poly- time computable function A function f : Σ ∗ ↦→ Σ ∗ is a poly-time computable function if there exists some poly-time TM M such that, on every input w, it halts with just f (w) on its tape. Definition: Poly- time reduction A language A is poly-time mapping reducible to language B, written A ≤P B, if there exists a poly-time computable function f : Σ∗ ↦→ Σ∗ such that, for every w ∈ Σ ∗: w ∈ A ⇐⇒ f (w) ∈ B Theorem If A ≤P B and B is in P, then A is in P. Proof We assume by hypothesis that M is an O(np)-time decider for B, and f is an O(n q)-time reduction from A to B. We can then construct a TM N as: In other words, on input w, it computes y = f (w), runs M on y and outputs whatever M outputs. f outputs something which is at most its running time, so |y| = O(|w| q). Thus, our machine runs in O(|w| pq). We have indeed construct a poly-time TM for our problem A, showing it is in P. □ Intuition This is another instance of the intuition behind reductions: if A reduces to B and B is easy, then A is easy. Corollary If A ≤P B and A is not in P, then B is not in P. Theorem: Tran- istivity If A ≤P B and B ≤P C, then A ≤P C. Proof The proof is completely analogous to the one we have just done: we can show that running fAB and then fBC runs in O(|w|pq). Definition: NP- hardness A language L is said to be NP-hard if every language L ′ in NP is such that: L ′ ≤P L Definition: NP- completeness A language L is said to be NP-complete if L is in NP and L is NP-hard. 42 5.3. POLYNOMIAL-TIME REDUCTIONS Notes by Joachim Favre Observation If any of the NP-complete language has a polynomial time decider, then every language in NP have a polynomial time decider and thus P = N P . Thus, NP-complete problems are the hardest NP problems. In other words, if we show that a problem is NP-complete, we showed that, even if we are not able to find an efficient algorithm, neither 40 years of research could. Cook-Levin Theorem SAT is NP-complete. Remark In practice, from now on, to show that a language is NP-complete we will always follow the same recipe. We first need to show that L belongs to NP, and thus we need to give a poly-time verifier for L. Then, for NP-hardness, we only need to show H ≤P L for some NP-complete language H. Indeed, since we know that ∀L ′ ∈ N P we have L ′ ≤P H and we show H ≤P L, we indeed get that L ′ ≤P L. Proposition: INDSET We can make the following reduction: SAT ≤P INDSET Since INDSET is in NP, this implies that INDSET is NP-complete. Proof We are given as input a CNF formula ϕ, and we want to turn it into a graph for INDSET in polynomial time. The idea is to construct a vertex per literal per clause, grouping the literals within a clause. We then link with an edge every vertex within a group (in brown) and every conflicting literals (so x and x; in blue). For instance, we could have: Our reduction outputs f (ϕ) = (G, m), where m is the number of clauses (m = 4 in our example, since we have K1, . . . , K4). This can definitely be done in polynomial time, so we now want to show that: ϕ ∈ SAT ⇐⇒ f (ϕ) ∈ INDSET Let’s suppose that ϕ ∈ SAT, and thus that there exists some satisfying assignment C of ϕ. In C, there is at least a true literal for each clause. For each group, we can take one of those literals, and construct an independent set of size m. Note that, because of the construction of f (ϕ), two vertices in this independent set cannot be linked by an edge. Indeed, we took a single variable per clause 43 Theory of computation CHAPTER 5. COMPLEXITY CLASSES (therefore there cannot be any brown edge), and we cannot have both xi and xi true (therefore there cannot be a blue edge either). We have thus indeed shown that ϕ ∈ SAT =⇒ f (ϕ) ∈ INDSET Let’s suppose that f (ϕ) ∈ INDSET, meaning that there exists an independent set C in G of size |C| = m. Again, by construction, C contains a vertex from each group. We can thus set the cor- responding literal to true, obtaining a satisfiable assignment for ϕ. Indeed, this allows to have one literal set to true by group, and no contradiction (it is impossible to need xi = xi = T , since there would be a blue edge between our elements and thus it would not be an independent set to start with). We have also shown that f (ϕ) ∈ INDSET =⇒ ϕ ∈ SAT, finishing our proof. □ Remark This proof is really beautiful: we managed to make a link between independent sets and satisfiable CNF formulas. Monday 1 st May 2023 — Lecture 9 : I hope the rest of the course will not be like this Definition: kSAT We define kSAT as: kSAT = {⟨ϕ⟩ | ϕ is satisfiable and each clause of ϕ contains at most k literals} Remark We notice that kSAT is in NP for any k, since we can use the exact same verifier as for SAT. Theorem: kSAT Let k ≥ 3. We can make the following reduction: SAT ≤P kSAT Since kSAT is in NP for all k, it is NP-complete for k ≥ 3. Proof We do the proof for k = 3, a general k ≥ 3 is completely analogous. We have some input ϕ, and we want to turn it into some input for 3SAT. Thus, let us say that ϕ has a clause with too many variables, for instance: K = (ℓ1 ∨ ℓ2 ∨ . . . ∨ ℓm), m > 3 We can replace this clause by the two following clauses, introducing a new variable z: K1 = (ℓ1 ∨ ℓ2 ∨ z), K2 = (z ∨ ℓ3 ∨ . . . ∨ ℓm) We notice replacing K by K1 ∧ K2 we keep a CNF, while preserving satisfiability. If K is satisfiable, we know that there is at least a satisfying assignment in ℓ1, ℓ2 or in ℓ3, . . . , ℓm; both cannot have no satisfying assignment at the same time. If both ℓ1 and ℓ2 are false, we can set z = T ; if all ℓ3, . . . , ℓm are false we can set z = T . On the other hand, if K1 ∧ K2 is satisfiable, it means that there must be a true literal in the clause where z or z is false, which allows K to be satisfied. We can do this iteratively, until all our clauses have at most 3 literals. Each clause needs to be iterated over at most a linear number of times, meaning that our reduction can indeed be done in polynomial time. Satisfiability is preserved throughout the transformation, finishing our reduction. □ 44 5.3. POLYNOMIAL-TIME REDUCTIONS Notes by Joachim Favre Remark This proof cannot be done for k = 2. Indeed, let’s say that we have: K = (x1 ∨ x2 ∨ x3) Now, we try doing our transformation on it, giving: K1 = (x1 ∨ z1), K2 = (z1 ∨ x2 ∨ x3) We have not reduced the number of literals in K2, so our proof cannot be applied. In fact, 2SAT is in P (this proof is non-trivial), and we don’t expect an NP-complete problem to reduce to a P problem (since it would imply P = NP). Definition: k- clique Let G be a graph. A k-clique is a subset of k vertices which are all pairwise connected. Definition: CLIQUE We define the language CLIQUE as: CLIQUE = {⟨G, k⟩ | G has a clique of size k} Example For instance, let us consider the following graph: Then, ⟨G, 3⟩ ∈ CLIQUE (a 3-clique is highlighted in red) but ⟨G, 4⟩ ̸∈ CLIQUE. Remark We can easily show that CLIQUE is in NP: we consider the certificate as a subset of vertices, and we verify that it is indeed a clique (which only requires Θ(k2) operations). Definition: Com- plement of graph Let G be a graph. The complement of G is: G = (V, E) where E is the complement of E, meaning: (u, v) ∈ E ⇐⇒ (u, v) ̸∈ E Theorem We can make the following reduction: INDSET ≤P CLIQUE Since CLIQUE is in NP, it means that CLIQUE is NP-complete. Proof We notice that a clique is the some kind of opposite to an independent set: all vertices are pairwise adjacent, instead of pairwise non- adjacent. Thus, let us consider the complement of G. We notice that, if we have some graph G = (V, E), then a subset S ⊆ V is an independent set of G if and only if S is a clique of G. Thus, we can take the following reduction: f (⟨G, k⟩) = 〈 G, k〉 By our observation, this indeed has the reduction property, finishing our proof. □ 45 Theory of computation CHAPTER 5. COMPLEXITY CLASSES Recall: Vertex incidence Let G = (V, E) be a graph. An edge (u1, u2) ∈ E is incident to a vertex v ∈ V if the vertex is an endpoint of the edge, meaning that: v = u1 or v = u2 Definition: Ver- tex cover Let G = (V, E) be a graph. A vertex cover is a subset S ⊆ V such that every edge of G is incident to at least one vertex in S. Definition: VER- TEX COVER language We define the following language: VERTEX COVER = {⟨G, k⟩ | G is a graph that has a vertex cover of size k} Example Let us consider the following graph: We notice that we have ⟨G, 4⟩ ∈ VERTEX COVER, the highlighted vertices are an example. However, ⟨G, 3⟩ ̸∈ VERTEX COVER. Indeed, we notice that a vertex has at most 3 incident edges in this graph. Thus, taking only 3 vertices gives at most 9 covered edges, but the graph has a total of 11 edges, so this is a contradiction. Remark We can notice rather trivially that this language is in NP: we can just consider the certificate as S, and see if this is indeed a vertex cover. Lemma Let G be a graph. S ⊆ V is a vertex cover if and only if S = V \\ S is an independent set. Proof =⇒ We do this proof by the contrapositive. If S is not an independent set, it means that there is an edge linking two of its vertices. However, this edge is not covered by a vertex of S and thus S is not a vertex cover. Proof ⇐= If S is an independent set, it means that none of its vertices are linked by an edge, and thus S is sufficient to cover all the edges.□ Corollary Let G be a graph, and k ∈ N. G has an independent set of size k if and only if G has a vertex cover of size n − k. Theorem We have the following reduction: INDSET ≤p VERTEX COVER Since VERTEX COVER is in NP, it implies that it is NP-complete. 46 5.3. POLYNOMIAL-TIME REDUCTIONS Notes by Joachim Favre Proof We can pick the following reduction: f (⟨G, k⟩) = ⟨G, n − k⟩ This reduction is indeed computable in polynomial time, and works thanks to our corollary. □ Definition: Set cover Let U = {1, . . . , n} and F = {T1, . . . , Tm} be a family of subsets (meaning that Ti ⊆ U for all i). A subset ⟨Ti1, . . . , Tik ⟩ ⊆ F is called a set cover of size k if: k⋃ j=1 Tij = U Definition: SET COVER lan- guage We define the following language: SET COVER = {⟨U, F, k⟩ | F contains a set cover of U of size k} Example Let us consider the following U and F: We notice that ⟨U, F, 3⟩ ∈ SET COVER since we can take: {{1, 2, 4}, {3, 6, 5}, {4, 7, 8, 9}} However, ⟨U, F, 2⟩ ̸∈ SET COVER. Indeed, we need a Ti containing 2, one containing 6 and one containing 9 (since they are each in an different set). We thus need at least 3 sets. NP We notice that this language is in NP: it is easy to verify that a given partition indeed works. Theorem We can make the following reduction: VERTEX COVER ≤P SET COVER Since SET COVER is in NP, it implies that SET COVER is NP-complete. Proof We notice that VERTEX COVER is a special case of SET COVER. The idea is to take U = E: we consider the edges as the element of our set, since we want to cover them. Then, to construct F, we construct sets Sv, which contain the set of edges which are incident to the vertex v ∈ V . Then, we can just take: F = {Sv | v ∈ V } For instance, it could give: 47 Theory of computation CHAPTER 5. COMPLEXITY CLASSES We notice that making a set cover is equivalent to making a vertex cover: each element of F represents a vertex, and thus making a set cover means that we found a set of vertices which covers the graph, and inversely. This reduction is also clearly computable in polynomial time. □ Monday 8 th May 2023 — Lecture 10 : More fun proofs Definition: Per- fect matchings Let G = (U ∪ V, E), where E ⊆ U × V , be a bipartite graph, meaning that there is some “left part” and some “right part” and edges can only go from left to right: U V We call M ⊆ E a matching if each v ∈ U ∪ V has at most one incident edge. It is moreover named a perfect matching (in red in the graph above) if each v ∈ U ∪ V has exactly one incident edge, which is equivalent to having M being a matching and |M | = |U | = |V |. Definition: PERFECT- MATCHING We define the following language: PERFECT-MATCHING = {⟨G⟩ | G admits a perfect matching} NP We notice that this problem is clearly in NP: if we are given some M , we can easily check that it is a matching and |M | = |U | = |V | in polynomial time. Complement We notice that the complement of PERFECT-MATCHING is also in NP. We can define the neighbourhood of some S ⊆ U as: N (S) = {v ∈ V | ∃u ∈ S, (u, v) ∈ E} In other words, it is the set of vertices reachable in V from S. Clearly, a very reasonable necessary condition for G not to admit any perfect matching is the existence of S such that if |S| > |N (s)|. However, Hall’s theorem states that this is also a sufficient condition: G contains a perfect matching if and only if S ≤ |N (S)| for all S ⊆ U . So, we can use our certificate as a subset of U , and check whether Hall’s condition is met. This is done in polynomial time, and indeed reaches the goals of certificates. Note that this is an open question, but many people think that, if both a language and its complement are in NP, then the language is in P; we have always managed to show that languages with this property where in P so far. In fact, if we managed to prove that this is false, we would have shown that P ̸= NP since P is closed under complement. Edmonds’ the- orem PERFECT-MATCHING is solvable efficiently: PERFECT-MATCHING ∈ P 48 5.3. POLYNOMIAL-TIME REDUCTIONS Notes by Joachim Favre Proof We can for instance use max flows, as seen in the Algorithms course. Definition: PERFECT-3- MATCHING Let G = (U ∪ V ∪ W, E), where E ⊆ U × V × W , be a tripartie graph. A perfect 3-matching is a subset M ⊆ E where each v ∈ U ∪ V ∪ W appears exactly once in M . We then naturally define: PERFECT-3-MATCHING = {G | G admits a perfect 3-matching} NP Again, this problem is definitely in NP. Theorem We have the following reduction: SAT ≤p PERFECT-3-MATCHING Since we know that PERFECT-3-MATCHING is in NP, it is also NP-complete. Proof Let ϕ be a CNF with n clauses and m different variables. The idea is to construct a graph using three types of vertices, with a total of 3mn vertices. The first n ones are in W , they each represent one of the clause. We then have mn − n other vertices in W , which will be helper ones. We finally make m groups of 2n vertices: each group represents a different variable, and half of their vertices are in U , whereas the other half is in V . The main thing to realise for this reduction is that we are able to link the vertices in each group using 2-edges from vertices of U to vertices of V so that there is only two choices for each variable (see the picture below, where vertices in green are in U , ones in red are in V and ones in blue are in W ): in a given group, either we always go clockwise (choosing green edges), or always go counterclockwise (choosing red edges). If we decide to go once clockwise, and once counterclockwise, then it necessarily means that a vertex will not be reached, or that it will be reached more than once. That way, we can encode the choice of setting xi = T xor xi = F . Then, we can extend each 2-edge into multiple 3-edges, by happening some vertices from W . First, we use a different edge representing xi = T to link to each clause where we have xi, and a different edge representing xi = F to each clause where we have xi. That way, to reach each of the vertices in W representing a clause, we need to make a (consistent) choice of variables to set to true. We then need to extend every 2-edge to reach every helper vertices in W , which allows variables not to appear in every clause. A partial drawing of the graph representing (x ∨ y)∧(x ∨ y)∧(x ∨ z) would look like: x ∨ y x ∨ y x ∨ z x y z 49 Theory of computation CHAPTER 5. COMPLEXITY CLASSES It is then possible to show that this reduction indeed has the reduction property: ϕ ∈ SAT ⇐⇒ f (ϕ) ∈ PERFECT-3-MATCHING Definition: SUBSET-SUM Let X be a multiset of positive integers (meaning that elements can appear multiple times in it). We define the following language: SUBSET-SUM = {⟨X, s⟩ | X contains a subset whose elements sum to s} Example For instance, let us consider: X = {1, 3, 4, 6, 13, 13} Then, we have ⟨X, 8⟩ ∈ SUBSET-SUM since we can take T = {1, 3, 4}. However, ⟨X, 12⟩ ̸∈ SUBSET-SUM. NP We can notice that this problem is in NP: if we are given a multiset of number, we can verify that it is indeed a subset and that its sum equals s in polynomial time. Remark We notice that we can encode the SUBSET-SUM problem in multiple ways: using binary (writing 1012 = 5) or unary (writing 111111 = 5). Using n bits we can encode number up to 2 n in binary, whereas we can only encode numbers up to n in unary. This is important because we measure the speed of some algorithm as a function of the size of the input. For instance, encoding everything in SUBSET-SUM using unary, makes it go in P: we can make an algorithm which is polynomial in the value of the input. However, as we will se right after, if we encode everything using binary, SUBSET-SUM is in fact NP-complete: there probably does not exist any algorithm which running time is a polynomial of the number of binary digits of the input. When we don’t mention anything, binary encoding is naturally implicit. Theorem We have the following reduction: PERFECT-3-MATCHING ≤p SUBSET-SUM Since SUBSET-SUM is in NP, it implies that it is NP-complete. Proof Let n = |E| be the number of edges. The idea is to encode each 3-edge e ∈ E ⊆ U × V × W as a 3n-bit number in base b = n + 1, using only 0s and 1s. Each vertex is represented by an n-digit number: it contains only zeroes except for a 1 at the i th position. Since each edge goes through three vertices, we are concatenating three n-bits numbers, and we thus indeed get that each edges are 3n-digits, and that they all have 3n − 3 zeroes and 3 ones. 1000 0100 0100 0100 1000 0010 0010 0001 1000 0001 0010 0001 0100 0010 1000 U V W 1 2 3 4 Now, we can define the SUBSET-SUM target to be a 3n-digits number containing only 1s. This models the fact that each node is picked exactly once. Let’s suppose that x ∈ PERFECT-3-MATCHING. We know that there is a set of edges which hits each vertex exactly once. This means that, if we choose to sum their binary encoding, then there 50 5.4. COOK-LEVIN THEOREM Notes by Joachim Favre will be exactly one 1 in each column and thus the sum will contain 3n ones. This indeed show that, then, f (x) ∈ SUBSET-SUM. Let’s now suppose that f (x) ∈ SUBSET-SUM. We may think that carry bits are a problem since, in binary, we would have 001 + 100 + 001 + 001 = 111 even though we did not choose a set of edges which yields a unique 1 in each column. However, since we chose to work in base |E| + 1, there cannot be any carry when adding at most |E| numbers which digits are all at most 1. This thus means that, indeed, we can interpret the result of SUBSET-SUM as a valid 3-matching, and thus x ∈ PERFECT-3-MATCHING. □ Observation We have seen the following reduction tree of NP-complete languages: PERFECT-3-MATCHING SET-COVER VERTEX-COVER SUBSET-SUM kSATINDSET SAT CLIQUE We can naturally use any of those NP-complete problems when we want to show another language is NP-complete. Monday 15 th May 2023 — Lecture 11 : I thought this would never happen 5.4 Cook-Levin theorem Remark We have not proven the Cook-Levin problem; that SAT is NP-complete. We will finish this course by doing this. Definition: WITNESS- EXISTENCE We define the following language: WITNESS-EXISTENCE = {〈V, x, 1 t〉 | ∃y, V (x, y) = 1 in t steps } where V is a verifier, x is an input to this TM, and 1 t is a running time t written in unary. Observation This problem is definitely in NP: we can just consider the certificate to be y. This indeed runs in polynomial time in the size of the input, since it does one step per digit of 1 t. We notice that we absolutely need t to be written using unary, binary would not have allowed to do this computation in polynomial time. Lemma: WITNESS- EXISTENCE WITNESS-EXISTENCE is NP-hard. Since moreover it is in NP, it is NP-complete. 51 Theory of computation CHAPTER 5. COMPLEXITY CLASSES Proof Let L ∈ NP be an arbitrary language in NP. We want to show that L ≤P WITNESS-EXISTENCE. By definition of NP, there exists a polynomial-time verifier V for L. In other words: L = { x | ∃y such that V (x, y) = 1 in time |x|k} Our reduction f is very trivial: f (x) = 〈V, x, 1(|x| k)〉 This can indeed be computed in polynomial time: we only need |x|k steps to write 1(|x| k), |x| steps to write x and a constant time to write V . The rest of the proof directly comes from the definition of this reduction and of WITNESS-EXISTENCE. □ Remark This problem is nice, but it is not very useful in this form. Let us thus introduce another concept to make it usable. Observation Any computer is done using electronics circuits. Let us try to formalise this, and see where it can bring us. Definition: Cir- cuit A circuit is some acyclic directed graph which nodes are input variables (which do not have any incoming edge) or gates (which can only consist in ∨, ∧ and ¬). The directed edges are wires. Finally, we have one or more output wires. Remark The fact that we are asking an acyclic graph may seem problematic, since we sometimes use the output of a circuit as its own input. However, this can easily be bypassed, since we can just copy the circuit multiple times. Definition: Size of a circuit The size of some circuit is its number of wires. Proposition: DNF Let f : {0, 1}n ↦→ {0, 1} be an arbitrary boolean function. Then, we can express f using a DNF of size O(2n). Proof The idea is to just convert the truth table of f using a DNF: f (x) = ⋁ y:f (y)=1 (x ? = y) = ⋁ y:f (y)=1 n⋀ i=1 {xi if yi = 1 xi if yi = 0 □ 52 5.4. COOK-LEVIN THEOREM Notes by Joachim Favre Proposition: CNF Let f : {0, 1}n ↦→ {0, 1} be an arbitrary boolean function. Then, we can express f using a CNF of size O(2n). Proof We can just construct the DNF of ¬f , and then take its negation. Every ∧ will be turned to an ∨, and inversely, thanks to Morgan’s Laws. This indeed implies that we get a CNF. Remark This is already great, but other circuits are often more expressive. We will in fact even show in exercises that there are some circuits which CNFs and DNFs are exponentially larger. Theorem Let M be some Turing machine, n ∈ N be an input length, and t(n) be its runtime. Then, we can compute in t(n) O(1) (meaning in time polynomial in t(n)) a circuit Cn of size O(t(n)2) such that: Cn(x) = M (x), ∀x ∈ {0, 1} n Proof sketch Let us consider a t × t table for M . It is constructed such that its i th row contains the configuration of the Turing machine at step i (recall that it encodes what is written on the tape, where the head is, and in what state the machine is in), including the input. In other words, the first row is an input to the circuit; the circuit then uses this input to propagate until the tth row, where we have the state of the machine which would run on this input at the tth step. Since the runtime is t, it means that the machine halted before iteration t. Moreover, it can only consider at most the t th first bits of input, since it considers a bit at a time. We can thus store everything important for the machine in this t × t table. Our goal is now to show that we can construct such a table efficiently using a circuit, which input variables are the n input squares in the first row. We notice that any cell in the table is determined by 4 cells below (meaning in the previous iteration of the machine). Indeed, if we don’t see the head in the four cells below, the cell cannot change. Else, the head can move in or out of this column, or modify it. However, this means that we can always make a circuit which can use the bits of 4 cells to produce the bits of the next cell. Putting such a circuit everywhere, we get a constant amount of wires for each cells, meaning that we get a circuit of size t2O(1) = O(t2), as required. Definition: CIRCUIT-SAT We define the following language: CIRCUIT-SAT = {⟨Cn⟩ | ∃x ∈ {0, 1} n such that Cn(x) = 1} where Cn is an arbitrary circuit. NP We notice that this problem is definitely in NP: we can simply consider the certificate to be x. Lemma: CIRCUIT-SAT We have the following reduction: WITNESS-EXISTENCE ≤p CIRCUIT-SAT Since WITNESS-EXISTENCE is NP-complete and CIRCUIT-SAT is in NP, this implies that CIRCUIT-SAT is NP-complete. Proof Let (V, x, 1 t) be an arbitrary input to WITNESS-EXISTENCE. The reduction makes the circuit CV from V using an input length |x| and simulating it for t steps. We hard-code x in the circuit, so 53 Theory of computation CHAPTER 5. COMPLEXITY CLASSES that this is only a function of y, yielding: f (V, x, 1 t) = C(y) = CV (x, y) where, by construction, CV (x, y) = V (x, y). The rest of the reduction is considered trivial and left as an exercise to the reader. □ Lemma: SAT We have the following reduction: CIRCUIT-SAT ≤p SAT Since CIRCUIT-SAT is NP-complete, it implies that SAT is NP-hard. Proof Let Cn be an arbitrary circuit. We notice that we are not allowed to convert the circuit to its CNF formula because the construction we have seen takes size O(2n). The idea instead is to introduce new variables, one for which wire of Cn: y1, . . . , ym. For instance, let us consider the following circuit: We want to force y1 = x2 ∨ y3, which can be done by using an equivalence, y1 ↔ y2 ∨ y3. We can thus make our reduction by forcing every wire to match its definition, and adding the condition that the output wire must be true: f (Cn) = y1 ∧ (y1 ↔ x1 ∧ x2) ∧ (y3 ↔ ¬x2) ∧ (y2 ↔ y1 ∨ y3) We notice that we can make a CNF for any y ↔ (x ∧ z), y ↔ (x ∧ z) and y ↔ ¬x in constant time. However, and-ing CNF formulas also yield a CNF formula. This means that we managed to turn our circuit into a CNF formula in linear time. The rest of the reduction is considered trivial and left as an exercise to the reader. □ Cook-Levin theorem SAT is NP-complete. Proof We have already seen that SAT is in NP, and we have just shown that it is NP hard. Thus, it is indeed NP-complete. , □ 54","libVersion":"0.5.0","langs":""}